{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO+LRSoDtO6bnLGpwzDLLjd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":21,"metadata":{"id":"u5SoWwjbHHbW","executionInfo":{"status":"ok","timestamp":1704886910840,"user_tz":-60,"elapsed":336,"user":{"displayName":"Filip Gąciarz","userId":"04108970981610913913"}}},"outputs":[],"source":["from tensorflow import keras\n","from keras.datasets import imdb\n","import matplotlib.pyplot as plt\n","from keras import layers\n","import tensorflow as tf"]},{"cell_type":"code","source":["vocab_size = 20000\n","maxlen = 200\n","(x_train, y_train), (x_val, y_val) = imdb.load_data(num_words = vocab_size)\n","x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"],"metadata":{"id":"Ul8NP3NHImL_","executionInfo":{"status":"ok","timestamp":1704886715656,"user_tz":-60,"elapsed":7253,"user":{"displayName":"Filip Gąciarz","userId":"04108970981610913913"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super(TokenAndPositionEmbedding, self).__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions"],"metadata":{"id":"lrMdde2IJFPg","executionInfo":{"status":"ok","timestamp":1704886337633,"user_tz":-60,"elapsed":452,"user":{"displayName":"Filip Gąciarz","userId":"04108970981610913913"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)"],"metadata":{"id":"TKpwbIwaJQpn","executionInfo":{"status":"ok","timestamp":1704886359827,"user_tz":-60,"elapsed":347,"user":{"displayName":"Filip Gąciarz","userId":"04108970981610913913"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["embed_dim = 32  # Embedding size for each token\n","num_heads = 2  # Number of attention heads\n","ff_dim = 32  # Hidden layer size in feed-forward network inside the transformer\n","\n","inputs = layers.Input(shape=(maxlen,))\n","embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n","x = embedding_layer(inputs)\n","transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n","x = transformer_block(x)\n","x = layers.GlobalAveragePooling1D()(x) #TODO add global average pooling\n","x = layers.Dropout(0.1)(x) #TODO add dropout\n","x = layers.Dense(20, activation = \"relu\")(x) #TODO add dense layer with about 20 neurons\n","x = layers.Dropout(0.1)(x) #TODO add dropout\n","outputs = layers.Dense(2, activation = \"softmax\")(x)#TODO add the final dense layer for classification\n","\n","model = keras.Model(inputs=inputs, outputs=outputs)"],"metadata":{"id":"i0kPZkWoJjEv","executionInfo":{"status":"ok","timestamp":1704886914761,"user_tz":-60,"elapsed":313,"user":{"displayName":"Filip Gąciarz","userId":"04108970981610913913"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","history = model.fit(\n","    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rLGVb62zLdQb","executionInfo":{"status":"ok","timestamp":1704887214076,"user_tz":-60,"elapsed":266439,"user":{"displayName":"Filip Gąciarz","userId":"04108970981610913913"}},"outputId":"7e525a46-1e59-4d69-a1d8-f0ac957d0eb7"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","782/782 [==============================] - 138s 172ms/step - loss: 0.3834 - accuracy: 0.8190 - val_loss: 0.2824 - val_accuracy: 0.8788\n","Epoch 2/2\n","782/782 [==============================] - 126s 161ms/step - loss: 0.1985 - accuracy: 0.9243 - val_loss: 0.3929 - val_accuracy: 0.8477\n"]}]}]}